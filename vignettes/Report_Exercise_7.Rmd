---
title: "Report_Exercise_7"
author: "Willy MÃ¼ller"
date: "2023-05-31"
output: html_document
---

#Exercise 1



```{r}
library(readr)
library(dplyr)
library(lubridate)

daily_fluxes <- read_csv("C:/Users/Admin/Documents/agds_report_willymuller/Data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>  
  
  
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |> 
  
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))
```


```{r}
lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes)
```

```{r}
library(caret)
library(tidyverse)

caret::train(
  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
  data = daily_fluxes |> drop_na(),  # drop missing values
  trControl = caret::trainControl(method = "none"),  # no resampling
  method = "lm"
)
```
```{r}
caret::train(
  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
  data = daily_fluxes |> drop_na(), 
  trControl = caret::trainControl(method = "none"),
  method = "knn"
)
```

```{r}
set.seed(123)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)
```

```{r}
plot_data <- daily_fluxes_train |> 
  dplyr::mutate(split = "train") |> 
  dplyr::bind_rows(daily_fluxes_test |> 
  dplyr::mutate(split = "test")) |> 
  tidyr::pivot_longer(cols = 2:9, names_to = "variable", values_to = "value")

plot_data |> 
  ggplot(aes(x = value, y = ..density.., color = split)) +
  geom_density() +
  facet_wrap(~variable, scales = "free")
```

```{r}
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())
```

```{r}

caret::train(
  pp, 
  data = daily_fluxes_train, 
  method = "knn",
  trControl = caret::trainControl(method = "none")
)
```

```{r}
daily_fluxes |> 
  summarise(across(where(is.numeric), ~quantile(.x, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE))) |> 
  t() |> 
  as_tibble(rownames = "variable") |> 
  setNames(c("variable", "min", "q25", "q50", "q75", "max"))
```


```{r}
pp_prep <- recipes::prep(pp, training = daily_fluxes_train) 
```

```{r}
daily_fluxes_juiced <- recipes::juice(pp_prep)
```

```{r}
daily_fluxes_baked <- recipes::bake(pp_prep, new_data = daily_fluxes_train)

# confirm that juice and bake return identical objects when given the same data
all_equal(daily_fluxes_juiced, daily_fluxes_baked)
```

```{r}
# prepare data for plotting
plot_data_original <- daily_fluxes_train |> 
  dplyr::select(one_of(c("SW_IN_F", "VPD_F", "TA_F"))) |> 
  tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = "var", values_to = "val")

plot_data_juiced <- daily_fluxes_juiced |> 
  dplyr::select(one_of(c("SW_IN_F", "VPD_F", "TA_F"))) |> 
  tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = "var", values_to = "val")

# plot density
plot_1 <- ggplot(data = plot_data_original, aes(val, ..density..)) +
  geom_density() +
  facet_wrap(~var)

# plot density by var
plot_2 <- ggplot(data = plot_data_juiced, aes(val, ..density..)) +
  geom_density() +
  facet_wrap(~var)

# combine both plots
cowplot::plot_grid(plot_1, plot_2, nrow = 2)
```

```{r}
visdat::vis_miss(
  daily_fluxes,
  cluster = FALSE, 
  warn_large_data = FALSE
  )
```

```{r}
pp |> 
  step_impute_median(all_predictors())
```

```{r}
pp |> 
  step_impute_knn(all_predictors(), neighbors = 5)
```


```{r}
# original data frame
df <- tibble(id = 1:4, color = c("red", "red", "green", "blue"))
df
```


```{r}
# after one-hot encoding
dmy <- dummyVars("~ .", data = df, sep = "_")
data.frame(predict(dmy, newdata = df))
```


```{r}
recipe(GPP_NT_VUT_REF ~ ., data = daily_fluxes) |> 
  step_dummy(all_nominal(), one_hot = TRUE)
```

```{r}
caret::nearZeroVar(daily_fluxes, saveMetrics = TRUE)
```


```{r}
pp |> 
  step_zv(all_predictors())
```

```{r}
plot_1 <- ggplot(data = daily_fluxes, aes(x = WS_F, y = ..density..)) +
  geom_histogram() +
  labs(title = "Original")

plot_2 <- ggplot(data = daily_fluxes, aes(x = log(WS_F), y = ..density..)) +
  geom_histogram() +
  labs(title = "Log-transformed")

cowplot::plot_grid(plot_1, plot_2)
```

```{r}
recipes::recipe(WS_F ~ ., data = daily_fluxes) |>   # it's of course non-sense to model wind speed like this
  recipes::step_log(all_outcomes())
```


```{r}
pp <- recipe(WS_F ~ ., data = daily_fluxes_train) |>
  step_BoxCox(all_outcomes())
```

```{r}
prep_pp <- prep(pp, training = daily_fluxes_train |> drop_na())
daily_fluxes_baked <- bake(prep_pp, new_data = daily_fluxes_test |> drop_na())
daily_fluxes_baked |>
  ggplot(aes(x = WS_F, y = ..density..)) +
  geom_histogram() +
  labs(title = "Box-Cox-transformed")
```

```{r}
recipe(WS_F ~ ., data = daily_fluxes) |>
  step_YeoJohnson(all_outcomes())
```
```{r}
# Data cleaning: looks ok, no obviously bad data
# no long tail, therefore no further target engineering
daily_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()
```

```{r}
# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```

```{r}
# make model evaluation into a function to reuse code
eval_model <- function(mod, df_train, df_test){
  
  # add predictions to the data frames
  df_train <- df_train |> 
    drop_na()
  df_train$fitted <- predict(mod, newdata = df_train)
  
  df_test <- df_test |> 
    drop_na()
  df_test$fitted <- predict(mod, newdata = df_test)
  
  # get metrics tables
  metrics_train <- df_train |> 
    yardstick::metrics(GPP_NT_VUT_REF, fitted)
  
  metrics_test <- df_test |> 
    yardstick::metrics(GPP_NT_VUT_REF, fitted)
  
  # extract values from metrics tables
  rmse_train <- metrics_train |> 
    filter(.metric == "rmse") |> 
    pull(.estimate)
  rsq_train <- metrics_train |> 
    filter(.metric == "rsq") |> 
    pull(.estimate)
  
  rmse_test <- metrics_test |> 
    filter(.metric == "rmse") |> 
    pull(.estimate)
  rsq_test <- metrics_test |> 
    filter(.metric == "rsq") |> 
    pull(.estimate)
  
  # visualise as a scatterplot
  # adding information of metrics as sub-titles
  plot_1 <- ggplot(data = df_train, aes(GPP_NT_VUT_REF, fitted)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
    labs(subtitle = bquote( italic(R)^2 == .(format(rsq_train, digits = 2)) ~~
                            RMSE == .(format(rmse_train, digits = 3))),
         title = "Training set") +
    theme_classic()
  
  plot_2 <- ggplot(data = df_test, aes(GPP_NT_VUT_REF, fitted)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
    labs(subtitle = bquote( italic(R)^2 == .(format(rsq_test, digits = 2)) ~~
                            RMSE == .(format(rmse_test, digits = 3))),
         title = "Test set") +
    theme_classic()
  
  out <- cowplot::plot_grid(plot_1, plot_2)
  
  return(out)
}

# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

```{r}
# KNN
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```


#Exercise 2
#Difference in training and test set scores:
#The bigger difference between the training and test set scores for the KNN model compared to the linear regression model suggests that the KNN model has higher variance. This means that the KNN model is more sensitive to the specific training data and can capture more complex patterns. As a result, the training data may be overfitted and it becomes difficult to transfer well to new unknown data.

#Evaluation on the test set:
#The evaluation on the test set, indicating better model performance of the KNN model compared to the linear regression model, suggests that the KNN model has lower bias. The ability of the KNN model to account for local patterns and adapt to the data can be helpful in capturing complex relationships that may not be captured by linear regression. However, it is important to consider the tradeoff, as the higher variance of the KNN model may lead to overfitting and reduced generalization performance.

#Position along the bias-variance tradeoff spectrum:
#Based on the observed differences, we can position the KNN model and the linear regression model along the bias-variance tradeoff spectrum as follows:

#Linear regression model: the linear regression model tends to have lower variance but possibly higher bias. It assumes a linear relationship between the predictors and the response variable, which might not capture more complex patterns in the data. Therefore, the linear regression model is generally biased toward lower variance and higher bias.
#KNN model: the KNN model tends to have higher variance but possibly lower bias. It adapts to local patterns in the data and can capture complex relationships. However, this flexibility can also lead to overfitting and higher sensitivity to the training data. Therefore, the KNN model is typically designed to have higher variance and a lower bias.
#When selecting a model, it is crucial to find a balance between bias and variance. The tradeoff between bias and variance implies that a decrease in bias can increase variance and vice versa. The choice between the KNN model and the linear regression model depends on the specific data set, the complexity of the relationships underlying it, and the tradeoff between overfitting and underfitting.



#Exercise 3

```{r}
daily_fluxes<- na.omit(daily_fluxes)
```

```{r}
library(ggplot2)

obs_mod_df <- data.frame(
  Date = daily_fluxes$TIMESTAMP,
  Observed_GPP = daily_fluxes$GPP_NT_VUT_REF,
  Linear_Reg_Model = predict(mod_lm, newdata = daily_fluxes),
  KNN_Model = predict(mod_knn, newdata = daily_fluxes)
)


line_plot <- ggplot(data = obs_mod_df, aes(x = Date)) +
  geom_line(aes(y = Observed_GPP, color = "Observed GPP"), size = 1) +
  geom_line(aes(y = Linear_Reg_Model, color = "Linear Regression Model"), size = 1) +
  geom_line(aes(y = KNN_Model, color = "KNN Model"), size = 1) +
  labs(x = "Date", y = "GPP", color = "Model") +
  scale_color_manual(values = c("Observed GPP" = "black", "Linear Regression Model" = "blue", "KNN Model" = "red")) +
  theme_classic()


line_plot

```


#The role of K:
#1. 
#k approaching 1: As k approaches 1, the model becomes more sensitive to the noise or variability in the data. With a smaller k, the model considers fewer neighbors to make predictions, resulting in a more flexible and potentially overfitting model. As a consequence: On the training set: The model will have a lower bias because it can capture the local patterns and fluctuations in the data. As a result, the R^2 on the training set will increase, indicating a better fit to the training data. On the test set: Since the model is more sensitive to noise, it may struggle to generalize well to unseen data. The model might perform poorly on the test set, resulting in a lower R^2 and higher MAE compared to the training set. This is due to the increased variance, as the model might fail to capture the underlying patterns in the data. 

#k approaching N: When k approaches the total number of observations in the data, the model becomes less flexible and more influenced by the overall structure of the data. With a larger k, the model considers a larger number of neighbors and averages their values, leading to a smoother decision boundary. As a consequence: On the training set: The model will have higher bias because it is constrained by a larger number of neighbors. Consequently, the R^2 on the training set might decrease as the model may not capture the local intricacies in the data. On the test set: A larger k helps in reducing the impact of noisy or outlier observations, resulting in improved generalization to unseen data. The model might have a higher R^2 and lower MAE on the test set compared to the training set. This is because the increased bias helps in reducing overfitting and achieving a better balance between bias and variance.










